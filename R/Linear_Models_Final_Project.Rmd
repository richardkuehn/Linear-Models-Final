---
title: "R Notebook"
output: html_notebook
---

```{r}
library(tidyverse)
library(ggcorrplot)
library(grid)
library(vcd)
```



```{r}
df = expectancy = read.csv('~/Downloads/linear_models_project_data.csv')
head(df)
```

### Dropping NA values
```{r}

df = replace(df, df=="NaN", NA)
df = df %>% drop_na()
df %>% head()
```


# data cleaning
```{r}
df = replace(df, df=="<=50K.", "<=50K")
df = replace(df, df==">50K.", ">50K")

df %>% head()
```
# Collapsing some of the categories in the educatioin column
```{r}
df$education = replace(df$education, df$education=="1st-4th", "Elementary/Middle")
df$education = replace(df$education, df$education=="5th-6th", "Elementary/Middle")
df$education = replace(df$education, df$education=="7th-8th", "Elementary/Middle")
df$education = replace(df$education, df$education=="Preschool", "Elementary/Middle")
df$education = replace(df$education, df$education=="Assoc-acdm", "Associates")
df$education = replace(df$education, df$education=="Assoc-voc", "Associates")
df$education = replace(df$education, df$education=="9th", "Highschool")
df$education = replace(df$education, df$education=="10th", "Highschool")
df$education = replace(df$education, df$education=="11th", "Highschool")
df$education = replace(df$education, df$education=="12th", "Highschool")
df$education = replace(df$education, df$education=="HS-grad", "Highschool")


df
```


### showing how many rows we have
```{r}
nrow(df)
```



# How does the distribution of income vary across different age groups?
```{r}

df$age_group <- cut(df$age, 
                      breaks = c(0, 20, 30, 40, 50, 60, 70, 80, Inf), 
                      labels = c("0-20", "21-30", "31-40", "41-50", "51-60", "61-70", "71-80", "81+"), 
                      right = FALSE)

df %>%
  group_by(income, age_group) %>%
  summarise(count = n()) %>%
  mutate(proportion = count / sum(count)) %>%
  ungroup() %>% 
  ggplot(aes(x = age_group, y = proportion, fill = income)) + 
  geom_bar(stat = 'identity', position = 'dodge')+theme(axis.text.x = element_text(angle = 45, hjust = 1)) + ggtitle("Proportion of Income Level per age group") + xlab("Age Group") + ylab("Proportion") + theme(plot.title = element_text(hjust = 0.5)) +
  labs(fill = "Income")

```



# What is the relationship between the number of hours worked per week and income level?
```{r}
df2 <- df
names(df2)[names(df2) == 'income'] <- 'Income'
df2 %>% ggplot(aes(x=hours.per.week, y=Income, color=Income, fill=Income))+geom_boxplot() + ggtitle("Hours worked each week per Income Level") + xlab("Hours worked per week") + ylab("Income Level") + theme(plot.title = element_text(hjust = 0.5))
```

# What is the relationship between the number of hours worked per week and income level?

```{r}
df %>% ggplot(aes(x=hours.per.week, y=income, color=income))+geom_violin()
```


# Converting income column into dummy variables
```{r}
data_dummies = fastDummies::dummy_cols(df, select_columns="income")
data_dummies
```

# Correlation between education number and income greater than 50K
```{r}
cor(data_dummies$education.num, data_dummies$`income_>50K`)
```

# Income level depending on education level

```{r}
df$education = factor(df$education, levels = c("Elementary/Middle", "Highschool", 'Associates', "Some-college", 'Bachelors','Prof-school', 'Masters', 'Doctorate'))

df %>%
  group_by(income, education) %>%
  summarise(count = n()) %>%
  mutate(proportion = count / sum(count)) %>%
  ungroup() %>% 
  ggplot(aes(x = education, y = proportion, fill = income)) + 
  geom_bar(stat = 'identity', position = 'dodge')+theme(axis.text.x = element_text(angle = 45, hjust = 1)) + ggtitle("Income Level proportion per Education Level") + xlab("Education Level") + ylab("Proportion") + theme(plot.title = element_text(hjust = 0.5)) +
  labs(fill = "Income")
```

**Interpretation:  As we can see from the above graph, the most amount of people who earns greater than 50K is people who have bachelors degree and this is because in our data we have more people who have bachelors degree than people who have Masters or Doctorate degree. Same idea applies to Highschool column, because we have more data on people whose highest level of education is highschool, that is why it looks people who earns less than 50K is in highschool column**



# Income level depending on occupation type

```{r}
df %>%
  group_by(income, occupation) %>%
  summarise(count = n()) %>%
  mutate(proportion = count / sum(count)) %>%
  ungroup() %>% 
  ggplot(aes(x = occupation, y = proportion, fill = income)) + 
  geom_bar(stat = 'identity', position = 'dodge')+theme(axis.text.x = element_text(angle = 45, hjust = 1)) + ggtitle("Income Level proportion per Occupation") + xlab("Occupation") + ylab("Proportion") + theme(plot.title = element_text(hjust = 0.5)) +
  labs(fill = "Income")
```

**Interpretation: As we can see from the bar graph above, we can say that the most amount of people who earns above 50K is people who has executive/managerial positions. After that it comes people who has a position in Prof/specialty. These makes sense because in real life as well, Executive managers earn a lot of money and Professors who make a lot of research can earn a lot of money. On the other hand, the most amount of people who make less than 50K is people who work in clerical jobs. These might include data entring, answering phone calls, filling paperwork and etc. **



# Correlation Matrix between all the numerical variables.

**Interpretation: As you can see in the correlation matrix, multicollinearity is not an issue becase there are no variables that are highly correlated with each other**

```{r}
cor_mat <- cor(df[,c('age',"education.num", 'capital.gain','capital.loss','hours.per.week')])
ggcorrplot(cor_mat,lab=TRUE, type='full')
```

```{r}
value_counts <- table(df$capital.loss)
df_capital.loss_count <- data.frame(value_counts)
df_capital.loss_count$pct <- round(100*df_capital.loss_count$Freq / sum(df_capital.loss_count$Freq),2)
df_capital.loss_count
```
```{r}
value_counts2 <- table(df$capital.gain)
df_capital.gain_count <- data.frame(value_counts2)
df_capital.gain_count$pct <- round(100*df_capital.gain_count$Freq / sum(df_capital.gain_count$Freq),2)
df_capital.gain_count
```
^ Might want to think about dropping these columns because they are highly skewed.

# Association between education level and income
```{r}

contingency_table = table(df$education, df$income)

chisq.test(contingency_table)


```

As we can see from the Chi-squared test, education and income has a very high association because the chi-square value is extremely high and p value is really low. This means that we have to reject null hypothesis and conclude that there is a high association between education and income.

```{r}
df
```



# Logistic Regression with Lasso and Cross Validation


# Train Test split by 80% training and 20% testing datasets
```{r}
library(glmnet)
library(caret)

X = model.matrix(income~0+., data=df)
X = scale(X)

y= df$income


train_index = sample(1:nrow(df), 0.8 * nrow(df))

train_X = X[train_index, ]
train_y = y[train_index]

test_X = X[-train_index, ]
test_y = y[-train_index]


```



# Running a 10 fold cross validation and finding out the best lambda
```{r}
set.seed(123)
cv_model = cv.glmnet(x=train_X, y=train_y, alpha=1, family="binomial", type.measure = "class", nfolds = 10)

best_lambda <- cv_model$lambda.min
best_lambda
```

```{r}
plot(cv_model)
```


```{r}
coefficient_matrix = as.matrix(coef(cv_model))
non_zero_indices <- which(coefficient_matrix != 0)


non_zero_coefs <- coefficient_matrix[non_zero_indices, ]


names_of_predictors = rownames(coefficient_matrix)[non_zero_indices]
#cbind(rownames(coefficient_matrix)[non_zero_indices], coefficient_matrix[coefficient_matrix!=0])
```


# Finalizing the model by using the best lambda found
```{r}
final_model <- glmnet(train_X, train_y, alpha = 1, family = "binomial", lambda = best_lambda)
```

# Casting the test_y data to be 1s and 0s
```{r}
#cbind(predicted_classes, test_y)
test_y = ifelse(test_y == ">50K", 1, 0)

```


# Make predictions and construct a confusion matrix where 0 represents people who earn less than or equal to 50K and 1 represents people who earn more than 50K
```{r}

predictions <- predict(final_model, newx = test_X, type = "response")
#predicted_classes <- ifelse(predictions > 0.5, ">50K", "<=50K")
predicted_classes <- ifelse(predictions > 0.5, 1, 0)


confusion <- confusionMatrix(factor(predicted_classes), factor(test_y))
print(confusion$table)

confusion_df <- data.frame(confusion$table)
confusion_df$Color <- with(confusion_df, ifelse(Prediction == Reference, "green", "red"))

ggplot(data = confusion_df, aes(x = factor(Prediction), y = factor(Reference), fill = Color)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Freq), color = "black", vjust = 1, size = 10) +
  scale_fill_manual(values = c("green" = "green", "red" = "red")) +
  theme_minimal() +
  labs(x = "Predicted", y = "Actual", fill = "Count") +
  ggtitle("Confusion Matrix Heatmap") +
  scale_y_discrete(limits = rev(levels(factor(confusion_df$Reference)))) + 
  theme(plot.title = element_text(hjust = 0.5))
```

# Finding the R squared value of this model
```{r}
print(paste("R squared value:", round(caret::R2(predicted_classes, test_y), 4)))
```


```{r}
tn = confusion$table[1, 1]
tp = confusion$table[2, 2]
fp = confusion$table[2, 1]
fn = confusion$table[1, 2]

precision <- tp / (tp + fp)
recall <- tp / (tp + fn)

# Calculate F1 Score
f1_score <- 2 * (precision * recall) / (precision + recall)
print(paste("Precision:", round(precision,3)))
print(paste("Recall:   ", round(recall, 3)))
print(paste("F1 Score: ", round(f1_score,3)))

```



# Calculate the Accuracy of this model
```{r}
accuracy <- mean(predicted_classes == test_y)
print(paste("Testing Accuracy:", round(100*accuracy, 3)))
```


# Linear Regression

```{r}
X = model.matrix(age~0+.-age_group, data=df)
X = scale(X)

y= df$age


train_index = sample(1:nrow(df), 0.8 * nrow(df))

train_X = X[train_index, ]
train_y = y[train_index]

test_X = X[-train_index, ]
test_y = y[-train_index]

```


# Building a 10 fold cross validation where response variable is age
```{r}
set.seed(123)
cv_model = cv.glmnet(x=train_X, y=train_y, alpha=1, nfolds = 10)

best_lambda <- cv_model$lambda.1se
best_lambda



```


# Building the final model
```{r}
final_model <- glmnet(train_X, train_y, alpha = 1, lambda = best_lambda)

```


# Names of the predictor variables
```{r}
coefficient_matrix = as.matrix(coef(cv_model))
non_zero_indices <- which(coefficient_matrix != 0)


non_zero_coefs <- coefficient_matrix[non_zero_indices, ]


names_of_predictors = rownames(coefficient_matrix)[non_zero_indices]
names_of_predictors
```

# Cut off point of Lasso Regression
```{r}
plot(glmnet(train_X, train_y, alpha = 1), xvar = "lambda")+abline(v=log(best_lambda))
```


# predicting the ages using the test set
```{r}
predictions <- predict(final_model, newx = test_X, type = "response")

```

# Showing RMSE, MAPE, R2
```{r}
print(paste("Root Mean Squared Error:", round(RMSE(predictions, test_y), 3)))
print(paste("Mean Absolute Percentage Error:", round(mean(abs((test_y-predictions)/test_y))*100,3)))
print(paste("R squared:", round(caret::R2(predictions, test_y),5)))
```



